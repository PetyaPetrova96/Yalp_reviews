{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181bbe29-26f4-4c4a-a9e9-0053263bff52",
   "metadata": {},
   "source": [
    "# Assignment: Scalable Processing\n",
    "## Yelp Reviews and Authenticity\n",
    "\n",
    "Large Scale Data Analysis | by Petya Petrova |pety@itu.dk | 27-02-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92690678-bb97-43d7-ab27-4a6f8db87079",
   "metadata": {},
   "source": [
    "## Connecting to the Spark Cluster job using the two JobParameters.json\n",
    "\n",
    "To connect this jupyter notebook with your Spark cluster, we need to tell jupyter how it can access the spark cluster. Below code accomplishes that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483a774-a0ca-4873-a2ee-51d2fcd30ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#Connecting to the Spark Cluster job using the two JobParameters.json\n",
    "#####################################################################\n",
    "    \n",
    "# Only execute this cell once.\n",
    "if '_EXECUTED_' in globals():\n",
    "    # check if variable '_EXECUTED_' exists in the global variable namespace\n",
    "    print(\"Already been executed once, not running again!\")\n",
    "else:\n",
    "    print(\"Cell has not been executed before, running...\")\n",
    "    import os, json, pyspark\n",
    "    from pyspark.conf import SparkConf\n",
    "    from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "    # Two files are automatically read: JobParameters.json for the Spark Cluster job using a temporary spark instance\n",
    "    # and JobParameters.json for the Jupyter Lab job to extract the hostname of the cluster. \n",
    "\n",
    "    MASTER_HOST_NAME = None\n",
    "\n",
    "    # Open the parameters Jupyter Lab app was launched with\n",
    "    with open('/work/JobParameters.json', 'r') as file:\n",
    "        JUPYTER_LAB_JOB_PARAMS = json.load(file)\n",
    "        # from pprint import pprint; pprint(JUPYTER_LAB_JOB_PARAMS) \n",
    "        for resource in JUPYTER_LAB_JOB_PARAMS['request']['resources']:\n",
    "            if 'hostname' in resource.keys():\n",
    "                MASTER_HOST_NAME = resource['hostname']\n",
    "\n",
    "    MASTER_HOST = f\"spark://{MASTER_HOST_NAME}:7077\"\n",
    "\n",
    "    conf = SparkConf().setAll([\n",
    "            (\"spark.app.name\", 'reading_job_params_app'), \n",
    "            (\"spark.master\", MASTER_HOST),\n",
    "        ])\n",
    "    spark = SparkSession.builder.config(conf=conf)\\\n",
    "                                .getOrCreate()\n",
    "\n",
    "    CLUSTER_PARAMETERS_JSON_DF = spark.read.option(\"multiline\",\"true\").json('/work/JobParameters.json')\n",
    "\n",
    "    # Extract cluster info from the specific JobParameters.json\n",
    "    NODES = CLUSTER_PARAMETERS_JSON_DF.select(\"request.replicas\").first()[0]\n",
    "    CPUS_PER_NODE = CLUSTER_PARAMETERS_JSON_DF.select(\"machineType.cpu\").first()[0] - 1\n",
    "    MEM_PER_NODE = CLUSTER_PARAMETERS_JSON_DF.select(\"machineType.memoryInGigs\").first()[0]\n",
    "\n",
    "    CLUSTER_CORES_MAX = CPUS_PER_NODE * NODES\n",
    "    CLUSTER_MEMORY_MAX = MEM_PER_NODE * NODES \n",
    "    \n",
    "    if CPUS_PER_NODE > 1:\n",
    "        EXECUTOR_CORES = CPUS_PER_NODE - 1  # set cores per executor on worker node\n",
    "    else:\n",
    "        EXECUTOR_CORES = CPUS_PER_NODE \n",
    "\n",
    "    EXECUTOR_MEMORY = int(\n",
    "        MEM_PER_NODE / (CPUS_PER_NODE / EXECUTOR_CORES) * 0.5\n",
    "    )  # set executor memory in GB on each worker node\n",
    "\n",
    "    # Make sure there is a dir for spark logs\n",
    "    if not os.path.exists('spark_logs'):\n",
    "        os.mkdir('spark_logs')\n",
    "\n",
    "    conf = SparkConf().setAll(\n",
    "        [\n",
    "            (\"spark.app.name\", 'spark_assignment'), # Change to your liking \n",
    "            (\"spark.sql.caseSensitive\", False), # Optional: Make queries strings sensitive to captialization\n",
    "            (\"spark.master\", MASTER_HOST),\n",
    "            (\"spark.cores.max\", CLUSTER_CORES_MAX),\n",
    "            (\"spark.executor.cores\", EXECUTOR_CORES),\n",
    "            (\"spark.executor.memory\", str(EXECUTOR_MEMORY) + \"g\"),\n",
    "            (\"spark.eventLog.enabled\", True),\n",
    "            (\"spark.eventLog.dir\", \"spark_logs\"),\n",
    "            (\"spark.history.fs.logDirectory\", \"spark_logs\"),\n",
    "            (\"spark.deploy.mode\", \"cluster\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ## check executor memory, taking into accout 10% of memory overhead (minimum 384 MiB)\n",
    "    CHECK = (CLUSTER_CORES_MAX / EXECUTOR_CORES) * (\n",
    "        EXECUTOR_MEMORY + max(EXECUTOR_MEMORY * 0.10, 0.403)\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        int(CHECK) <= CLUSTER_MEMORY_MAX\n",
    "    ), \"Executor memory larger than cluster total memory!\"\n",
    "\n",
    "    # Stop previous session that was just for loading cluster params\n",
    "    spark.stop()\n",
    "\n",
    "    # Start new session with above config, that has better resource handling\n",
    "    spark = SparkSession.builder.config(conf=conf)\\\n",
    "                                .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    _EXECUTED_ = True\n",
    "    print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbb4de-8b40-4364-b8d3-2cc488b16cbf",
   "metadata": {},
   "source": [
    "Click on the \"SparkMonitor\" tab at the top in Jupyter Lab to see the status of running code on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de3dec-d95c-44b9-a996-12e97cc34c2b",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "Here we specify where the yelp datasets are located on UCloud and read then using the spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa96702-5902-4482-9cd2-77d6f5da0f75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in the business and review files\n",
    "# This is the path to the shared datasets provided by adding an the dataset input folder\n",
    "# when submitting the spark cluster job.\n",
    "business = spark.read.json('file:////work/yelp/yelp_academic_dataset_business.json') # Use the file:/// prefix to indicate we want to read from the cluster's filesystem\n",
    "business = business.persist()\n",
    "# Persist 2 commonly used dataframes since they're used for later computations\n",
    "# https://sparkbyexamples.com/spark/spark-difference-between-cache-and-persist/\n",
    "\n",
    "users = spark.read.json(\"file:////work/yelp/yelp_academic_dataset_user.json\")\n",
    "\n",
    "reviews = spark.read.json('file:////work/yelp/yelp_academic_dataset_review.json')\n",
    "reviews = reviews.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f6184-410e-4a04-a4a1-7b080353dc1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get number of rows with no sampling:\n",
    "reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20087cdb-d6f2-4aab-9fe2-bf3f072044db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#business.show()\n",
    "# Get only the column names\n",
    "#column_names = reviews.columns\n",
    "#print(column_names)\n",
    "# Get only the first row\n",
    "first_row = reviews.head(1)\n",
    "print(first_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07983acb-15d8-45ea-9d12-3bc8759280b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter to only Arizona businesses with \"Mexican\" as part of their categories\n",
    "az_mex = business.filter(business.state == \"AZ\")\\\n",
    "                .filter(business.categories.rlike(\"Mexican\"))\\\n",
    "                .select(\"business_id\", \"name\")\n",
    "\n",
    "# Join with the reviews\n",
    "az_mex_rs = reviews.join(az_mex, on=\"business_id\", how=\"inner\")\n",
    "\n",
    "# Filter to only 5 star reviews\n",
    "good_az_mex_rs = az_mex_rs.filter(az_mex_rs.stars == 5)\\\n",
    "                        .select(\"name\",\"text\")\n",
    "\n",
    "# Print the top 20 rows of the DataFrame\n",
    "good_az_mex_rs.show()\n",
    "\n",
    "# Convert to pandas (local object) and save to local file system\n",
    "good_az_mex_rs.toPandas().to_csv(\"good_az_reviews.csv\", header=True, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d48f9a-cfb3-41e6-a8a1-91e07dca1147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Begining of the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "643d8762-7549-445f-841c-7b9eb6ac2b39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bu = spark.read.json('file:////work/yelp/yelp_academic_dataset_business.json') \n",
    "bu = business.persist()\n",
    "\n",
    "us = spark.read.json(\"file:////work/yelp/yelp_academic_dataset_user.json\")\n",
    "\n",
    "re = spark.read.json('file:////work/yelp/yelp_academic_dataset_review.json')\n",
    "re = reviews.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750f0226-9212-4564-8ede-8a3cddb678af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EDA\n",
    "bu.show(1)\n",
    "#us.show(1)\n",
    "#re.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d7652ad-0c57-43ed-a1f1-f9523a92c888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#3.1 Specific DataFrame Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5841da57-f731-4821-bb76-f1d5c8bcd3b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1. Find the total number of reviews for all businesses. The output should be in the form of a Spark Table/DataFrame with one value representing the count.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "# Sum the values in the review_count column\n",
    "total_review_count = bu.agg(spark_sum(\"review_count\")).collect()[0][0]\n",
    "\n",
    "# Print the result\n",
    "print(\"Total Review Count:\", total_review_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840f8b8-de73-423e-9500-dd2cb3260ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.Find all businesses that have received 5 stars and that have been reviewed by 500 or more users. The output should be in the form of DataFrame of (name, stars, review count).\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "businesses = bu.filter((col(\"stars\") == 5) & (col(\"review_count\") >= 500)) \\\n",
    "               .select(\"name\", \"stars\", \"review_count\")\n",
    "\n",
    "# Show the DataFrame of businesses with 5 stars and 500 or more reviews\n",
    "businesses.show()\n",
    "\n",
    "# Count the businesses with 5 stars and 500 or more reviews\n",
    "count_businesses = businesses.count()\n",
    "\n",
    "# Print the result\n",
    "print(\"Number of businesses with 5 stars and 500 or more reviews:\", count_businesses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c90db-92b7-4bdc-a924-1b0684e53407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Find the influencers who have written more than 1000 reviews. The output should be in the form of a Spark Table/DataFrame of user id. Find the businesses that have been\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "influencers = us.filter(col(\"review_count\") > 1000).select(\"user_id\")\n",
    "\n",
    "# Count the influencers who have written more than 1000 reviews.\n",
    "count_influencers = influencers.count()\n",
    "\n",
    "# Print the result\n",
    "print(\"Number of influencers who have written more than 1000 reviews:\", count_influencers)\n",
    "influencers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeff0be-0af4-49d7-b919-9ecd4738a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_reviews_users = bu.join(re, \"business_id\").join(influencers, \"user_id\")\n",
    "business_count = business_reviews_users.groupby(\"business_id\").agg(f. countDistinct(\"user_id\").alias(\"infl_count\"))\n",
    "bus_5_infl = business_count.filter(business_count.infl_count > 5).select(\"business_id\")\n",
    "\n",
    "bus_5_infl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b1f85-e985-4b6d-afe4-c00c5568defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_reviews_users = bu.join(re, \"business_id\").join(influencers, \"user_id\")\n",
    "business_count = business_reviews_users.groupby(\"business_id\").agg(f. countDistinct(\"user_id\").alias(\"infl_count\"))\n",
    "bus_5_infl = business_count.filter(business_count.infl_count > 5).select(\"business_id\")\n",
    "bus_5_infl.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476f527-4eeb-4d05-b19c-e79effb504b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#3.Find the influencers who have written more than 1000 reviews. The output should be in the form of a Spark Table/DataFrame of user id. Find the businesses that have been\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "influencers = us.filter(col(\"review_count\") > 1000).select(\"user_id\")\n",
    "\n",
    "# Show the DataFrame of influencers' user IDs\n",
    "#influencers.show()\n",
    "\n",
    "# Count the influencers who have written more than 1000 reviews.\n",
    "count_influencers = influencers.count()\n",
    "\n",
    "# Print the result\n",
    "print(\"Number of influencers who have written more than 1000 reviews:\", count_influencers)\n",
    "\n",
    "\n",
    "#3.Find the businesses that have been reviewed by more than 5 influencer users.\n",
    "\n",
    "\n",
    "\n",
    "# Join business, reviews, and corresponding influencers using an inner join\n",
    "business_review_influencers = business.join(reviews, \"business_id\").join(influencers, \"user_id\")\n",
    "\n",
    "# For each business, count the number of unique influencers\n",
    "business_influencer_count = business_review_influencers.groupby(\"business_id\").agg(f.countDistinct(\"user_id\").alias(\"count_influencers\"))\n",
    "\n",
    "# Filter businesses with more than 5 unique influencers and select the business_id\n",
    "businesses_with_more_than_5_influencers = business_influencer_count.filter(business_influencer_count.count_influencers > 5).select(\"business_id\")\n",
    "\n",
    "# Count the number of businesses with more than 5 unique influencers\n",
    "count_businesses = businesses_with_more_than_5_influencers.count()\n",
    "\n",
    "print(f'Number of businesses with more than 5 unique influencers: {count_businesses}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e6cd8f-d39f-4889-a9b8-28f0f78474ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#4. Find an ordered list of users based on the average star counts they have given in all their reviews.\n",
    "\n",
    "# Compute the average star counts given by users in all their reviews\n",
    "review_user_avg_stars = re.join(us, \"user_id\").groupby(\"user_id\").mean(\"stars\").sort(\"avg(stars)\", ascending=False)\n",
    "\n",
    "# Show the ordered list of users based on average star counts\n",
    "review_user_avg_stars.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba41230d-be4d-47d3-9951-5252107dde0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#3.2.1 Data Exploration\n",
    "#1.  A) What is the percentage of reviews containing a variant of the word \"authentic\"?\n",
    "\n",
    "# Filter out the businesses to be only restaurants\n",
    "restaurants = business[business.categories.contains('Restaurants')]\n",
    "restaurant_reviews = restaurants.join(reviews, \"business_id\")\n",
    "\n",
    "# Filter reviews containing the word \"authentic\"\n",
    "authentic_reviews = restaurant_reviews.filter(restaurant_reviews.text.rlike('authentic'))\n",
    "\n",
    "# Get the percentage of such reviews\n",
    "authentic_percentage = (authentic_reviews.count() / restaurant_reviews.count()) * 100\n",
    "\n",
    "# Print the percentage\n",
    "print(f'{round(authentic_percentage, 2)} percentage of reviews contain the word \"authentic\"')\n",
    "\n",
    "\n",
    "#1. B) How many reviews contain the string \"legitimate\" grouped by type of cuisine?\n",
    "# Filter reviews with the word \"legitimate\"\n",
    "legitimate_reviews = restaurant_reviews.filter(restaurant_reviews.text.rlike('legitimate'))\n",
    "count_legitimate_reviews = legitimate_reviews.count()\n",
    "\n",
    "print(f'Reviews containing the word \"legitimate\": {count_legitimate_reviews}')\n",
    "\n",
    "# Calculate the percentage of reviews containing the word \"legitimate\"\n",
    "legitimate_percentage = (count_legitimate_reviews / restaurant_reviews.count()) * 100\n",
    "\n",
    "print(f'{round(legitimate_percentage, 2)} percentage of reviews contain the word \"legitimate\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1807acc-a963-4523-b2af-c5ac5b484c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# QUESTION\n",
    "# Add a column indicating whether the review contains authenticity language\n",
    "restaurant_reviews = restaurant_reviews.withColumn(\"contains_authenticity\", restaurant_reviews.text.rlike('authentic|legitimate'))\n",
    "restaurant_reviews_cube = restaurant_reviews.cube(\"state\", \"city\", \"contains_authenticity\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add column mentioning whether the review includes some NEGATIVE words\n",
    "restaurant_reviews = restaurant_reviews.withColumn(\"contains_negative\", restaurant_reviews.text.rlike('(dirty) |(cheap) | (rude)'))\n",
    "\n",
    "# Add column mentioning whether the review includes some POSITIVE words\n",
    "restaurant_reviews = restaurant_reviews.withColumn(\"contains_positive\", restaurant_reviews.text.rlike('(nice) | (fresh) | (eleg)'))\n",
    "\n",
    "# Get reviews with authentic language and negative/positive language\n",
    "restaurant_reviews_neg = restaurant_reviews.filter((restaurant_reviews.contains_authenticity) & (restaurant_reviews.contains_negative))\n",
    "restaurant_reviews_pos = restaurant_reviews.filter((restaurant_reviews.contains_authenticity) & (restaurant_reviews.contains_positive))\n",
    "\n",
    "# Look at the categories\n",
    "# * All\n",
    "category_count_all = restaurant_reviews.withColumn('category', F.explode(F.split(F.col('categories'), ', '))) \\\n",
    "    .groupBy('category').count().withColumnRenamed(\"count\", \"count_all\")\n",
    "\n",
    "# * Negative\n",
    "restaurant_reviews_neg_count = restaurant_reviews_neg.withColumn('category', F.explode(F.split(F.col('categories'), ', '))) \\\n",
    "    .groupBy('category').count().withColumnRenamed(\"count\", \"count_neg\")\n",
    "restaurant_reviews_neg_count = restaurant_reviews_neg_count.join(category_count_all, \"category\") \\\n",
    "    .withColumn(\"normalized\", (F.col(\"count_neg\") / F.col(\"count_all\")) * 100)\n",
    "\n",
    "# * Positive\n",
    "restaurant_reviews_pos_count = restaurant_reviews_pos.withColumn('category', F.explode(F.split(F.col('categories'), ', '))) \\\n",
    "    .groupBy('category').count().withColumnRenamed(\"count\", \"count_pos\")\n",
    "restaurant_reviews_pos_count = restaurant_reviews_pos_count.join(category_count_all, \"category\") \\\n",
    "    .withColumn(\"normalized\", (F.col(\"count_pos\") / F.col(\"count_all\")) * 100)\n",
    "\n",
    "# Save the results\n",
    "restaurant_reviews_neg_count.orderBy(\"normalized\", ascending=True).show()\n",
    "restaurant_reviews_pos_count.orderBy(\"normalized\", ascending=True).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d5bfd-1fb1-4530-a91e-593c8a3db071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2. • Is there a difference in the amount of authenticity language used in the different areas?\n",
    "# Add a column indicating whether the review contains authenticity language\n",
    "restaurant_reviews = restaurant_reviews.withColumn(\"contains_authenticity\", restaurant_reviews.text.rlike('authentic|legitimate'))\n",
    "\n",
    "# Group by state and city, and count the number of reviews containing authenticity language\n",
    "region_authenticity_counts = restaurant_reviews.groupBy(\"state\", \"city\").agg(\n",
    "    f.sum(f.col(\"contains_authenticity\").cast(\"int\")).alias(\"authenticity_count\"),\n",
    "    f.count(\"*\").alias(\"total_reviews\")\n",
    ")\n",
    "\n",
    "# Calculate the percentage of reviews containing authenticity language for each region\n",
    "region_authenticity_counts = region_authenticity_counts.withColumn(\n",
    "    \"authenticity_percentage\", \n",
    "    f.col(\"authenticity_count\") / f.col(\"total_reviews\") * 100\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "region_authenticity_counts.show(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca31c00-c275-426e-9637-18227cb58738",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2.2 Hypothesis TestingNull Hypothesis \n",
    "#(H0):There is no significant difference in the relationship between authenticity language and typically negative words in restaurants serving South American or Asian cuisine compared to restaurants serving European cuisine.\n",
    "\n",
    "#(H1):There is a significant difference in the relationship between authenticity language and typically negative words in restaurants serving South American or Asian cuisine compared to restaurants serving European cuisine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082c70c-f8bf-48e0-88af-b3b483cd7ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the DataFrame for South American, Asian, and European cuisines\n",
    "cuisine_filtered = re.filter((col('categories').contains('South American')) | \n",
    "                                  (col('categories').contains('Asian')) | \n",
    "                                  (col('categories').contains('European')))\n",
    "\n",
    "# Count the number of reviews containing the word \"authentic\" for each cuisine type\n",
    "authentic_counts_by_cuisine = cuisine_filtered.groupBy('categories').agg(\n",
    "    count(when(col('text').rlike('authentic'), 1)).alias('authentic_count_by_cuisine')\n",
    ")\n",
    "\n",
    "# Count the number of reviews containing the word \"legitimate\" for each cuisine type\n",
    "legitimate_counts_by_cuisine = cuisine_filtered.groupBy('categories').agg(\n",
    "    count(when(col('text').rlike('legitimate'), 1)).alias('legitimate_count_by_cuisine')\n",
    ")\n",
    "\n",
    "# Join the counts for both words by cuisine type\n",
    "counts_by_cuisine = authentic_counts_by_cuisine.join(legitimate_counts_by_cuisine, 'categories')\n",
    "\n",
    "# Calculate the percentage of reviews containing each word for each cuisine type\n",
    "counts_by_cuisine = counts_by_cuisine.withColumn('authentic_percentage', \n",
    "                                                 (col('authentic_count_by_cuisine') / rest_rs.count()) * 100)\n",
    "counts_by_cuisine = counts_by_cuisine.withColumn('legitimate_percentage', \n",
    "                                                 (col('legitimate_count_by_cuisine') / rest_rs.count()) * 100)\n",
    "\n",
    "# Display the results\n",
    "counts_by_cuisine.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5850e4f4-76b6-47b3-a2a5-46c4fc7037f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = bu.filter(bu.categories.rlike(\"Restaurants\"))\n",
    "restaurants.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc371fa5-f2f8-401f-a12a-19f26a1565cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a3ad0f-93a1-4261-a17b-b3589972ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.select('postal_code').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8433fc5a-0933-47ec-a8a5-9f61627e2d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bu = spark.read.json('file:////work/yelp/yelp_academic_dataset_business.json') \n",
    "#bu = business.persist()\n",
    "\n",
    "us = spark.read.json(\"file:////work/yelp/yelp_academic_dataset_user.json\")\n",
    "\n",
    "re = spark.read.json('file:////work/yelp/yelp_academic_dataset_review.json')\n",
    "#re = reviews.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09999652-8bbf-4253-9c4e-1acc7bbf2b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = bu.filter(bu.categories.rlike(\"Restaurants\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62998d4b-e228-4a41-8b25-c643565674b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out all the restaurants \n",
    "all_df = re.join(restaurants , re['business_id'] == restaurants ['business_id'], 'inner') \\\n",
    "          .join(us, re['user_id'] == us['user_id'], 'inner') \\\n",
    "          .select(re['stars'], re['text'],restaurants['categories'],restaurants['state'],restaurants['city'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cfc8d-b92f-4402-b930-830196df071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "237d8cb1-c256-4cfa-b4e8-4f5a5f2ebb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how the words authentic and legitimate are compared to each other \n",
    "# Filter reviews that contain the words \"authentic\" and \"legitimate\"\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "filtered_df = all_df.filter(col(\"text\").contains(\"authentic\") | col(\"text\").contains(\"legitimate\"))\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "#filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "277c62b6-6e99-4d5a-b7bc-effedbe45828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by city and count the occurrences\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "cube_df = filtered_df.cube(\"state\", \"city\").agg(count(\"*\").alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a522c2-458b-46a1-bd26-74080363d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cube_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a96f378-a681-419f-9679-1ae78e207364",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cube_df = cube_df.orderBy(col(\"count\").desc())\n",
    "#sorted_cube_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30730618-020e-49bc-b7f5-7c0c9b636b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where state or city is null\n",
    "filtered_sorted_cube_df = sorted_cube_df.filter(col(\"state\").isNotNull() & col(\"city\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38742c51-9341-4974-be5d-74cd673827a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_sorted_cube_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cc27c13-affc-4060-b131-754b8be6ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the specific row with state=\"CA\" and city=\"Santa Barbara\"\n",
    "filtered_sorted_cube_df = filtered_sorted_cube_df.filter((col(\"state\") != \"CA\") | (col(\"city\") != \"Santa Barbara\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f319be-dece-4279-aad7-5326a67ea355",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c852803-f429-4a6a-b59e-7fc930ad0fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row_categories = all_df.select(\"categories\").first()[0]\n",
    "#print(first_row_categories)\n",
    "# Take the second row of categories\n",
    "second_row_categories = all_df.select(\"categories\").collect()[1][0]\n",
    "\n",
    "# Print the second row of categories\n",
    "#print(\"Second row of categories:\", second_row_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf2add-ad32-49fb-bec3-c9830c27cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92b1e5-bfa2-4130-a88f-0cb06c6e3b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame containing French, Italian, Mediterranean, or European categories\n",
    "\n",
    "\n",
    "\n",
    "european_df = filtered_df.filter((col(\"categories\").contains(\"French\")) |    (col(\"categories\").contains(\"Italian\")) |    (col(\"categories\").contains(\"Mediterranean\")) |\n",
    "    (col(\"categories\").contains(\"European\")))\n",
    "\n",
    "# DataFrame containing Thai, Japanese, Chinese, Korean, Indian, Mexican, Soul, Latin American, and Asian categories\n",
    "\n",
    "asian_df = filtered_df.filter(\n",
    "    (col(\"categories\").contains(\"Thai\")) |\n",
    "    (col(\"categories\").contains(\"Japanese\")) |\n",
    "    (col(\"categories\").contains(\"Chinese\")) |\n",
    "    (col(\"categories\").contains(\"Korean\")) |\n",
    "    (col(\"categories\").contains(\"Indian\")) |\n",
    "    (col(\"categories\").contains(\"Mexican\")) |\n",
    "    (col(\"categories\").contains(\"Soul\")) |\n",
    "    (col(\"categories\").contains(\"Latin American\")) |\n",
    "    (col(\"categories\").contains(\"Asian\"))\n",
    ")\n",
    "\n",
    "# Show the DataFrames\n",
    "print(\"European DataFrame:\")\n",
    "#european_df.show()\n",
    "\n",
    "#print(\"Asian DataFrame:\")\n",
    "#asian_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5003290-3c85-4724-b0df-ac4f7be997a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampled_european_df = european_df.sample(False, 0.1)  \n",
    "sampled_european_df = sampled_european_df.limit(50)\n",
    "\n",
    "sampled_asian_df = asian_df.sample(False, 0.1)  \n",
    "sampled_asian_df = sampled_asian_df.limit(50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd0efb-79da-4f8c-88fa-940118e67966",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average sentiment scores for Asian cuisines\")\n",
    "print(\"Positive score: {}\".format(0.20496368239812873))\n",
    "print(\"Negative score: {}\".format(-0.017286704808065025))\n",
    "print(\"\\nAverage sentiment scores for European cuisines:\")\n",
    "print(\"Positive score: {}\".format(0.27702170666841436))\n",
    "print(\"Negative score: {}\".format(-0.029861111111111116))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3571cddf-4132-43fc-a37c-095081c9b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_european_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c452e6e7-bb9f-4aa6-b227-fa5f83e08e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Select the \"text\" column containing blob data\n",
    "blob_data_european_df = sampled_european_df.select(col(\"text\"))\n",
    "blob_data_asian_df = sampled_asian_df.select(col(\"text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49ef7b75-301d-44b7-9057-543c61c16c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save European DataFrame text data to a text file\n",
    "blob_data_european_df.write.text(\"european_text_data.txt\")\n",
    "\n",
    "# Save Asian DataFrame text data to a text file\n",
    "blob_data_asian_df.write.text(\"asian_text_data.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2753c3c-1972-4c7b-874e-8f3f4f5aed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sampled European DataFrame to CSV\n",
    "#sampled_european_df.write.csv(\"sampled_european.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# Save sampled Asian DataFrame to CSV\n",
    "#sampled_asian_df.write.csv(\"sampled_asian.csv\", header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "723f4307-aaf7-4fe0-8945-4ac346cc0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "633d6d80-6690-4734-bf8d-8d5b81a06c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8a7793-69a1-47ca-910d-500847ca5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55aa4e2f-a0ee-4ed6-9157-ed5e08782e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate polarity score using VADER\n",
    "def get_sentiment_score(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment = sid.polarity_scores(text)\n",
    "    return sentiment[\"compound\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f0330c2-a5ea-4a24-aa56-cf3949ddbb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define UDF for sentiment analysis\n",
    "sentiment_udf = udf(get_sentiment_score, FloatType())\n",
    "\n",
    "# Add polarity score to Asian DataFrame\n",
    "asian_df_with_sentiment = asian_df.withColumn(\"sentiment_score\", sentiment_udf(col(\"text\")))\n",
    "\n",
    "# Add polarity score to European DataFrame\n",
    "european_df_with_sentiment = european_df.withColumn(\"sentiment_score\", sentiment_udf(col(\"text\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2b6c2aaf-e90f-478b-9033-307172847b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = all_df.limit(1500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d479bc-1083-4823-adb5-36f9ecf6374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ef8ea-e3a4-4175-8ecf-71d7aa54f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml. regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql. functions import col\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e19cc896-601a-43d9-ba49-9ca1e92dd419",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "reviews_tokenized = tokenizer. transform(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7c6fa40d-b713-478e-a413-118d7a39f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "be0cab6c-492d-4a7d-8978-3e43a3ef4c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token to Numerical Features\n",
    "tokens2numb = HashingTF(inputCol=\"tokens\", outputCol=\"NewFeatures\", numFeatures=500)\n",
    "#transformer = IDF(inputCol=\"NewFeatures\", outputCol=\"NewFeatures1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ae268-2b37-492b-ae74-dd7db8efc616",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = transformer.fit(tokens2numb.transform(reviews_tokenized))\n",
    "transformed_data = model.transform(tokens2numb.transform(reviews_tokenized))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "878bde64-3141-4ce5-be62-b80f62fdd98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the transformed data\n",
    "transformed_data= transformed_data.drop(\"NewFeatures1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a50d94-c510-4133-9064-5fe850278674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the transformed data\n",
    "transformed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bbc262f0-713e-40aa-96d2-7d5fcf2a2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"NewFeatures\"], outputCol=\"features_vector1\")\n",
    "transformed_data = assembler.transform(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6325bec-f1c0-4960-b1a1-f1b41f080dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data.select(\"features_vector1\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4e7c5e42-7133-4a7d-8fa5-5ef9c72bfa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Split the data  (60%, 20%, 20%)\n",
    "train_data, test_data, dev_data = transformed_data.randomSplit([0.6, 0.2, 0.2], seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de691ed-0dbe-49ce-85f6-85d850f25190",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763060de-af50-4427-9518-bf0c0ce15a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d3f2c-2322-4baa-b031-d56c4b4369e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns except the target variable (stars), NewFeatures, and features\n",
    "feature_columns = [col for col in train_data.columns if col not in [\"stars\", \"NewFeatures\", \"features\",\"tokens\",\"text\"]]\n",
    "\n",
    "# Use selected columns as features\n",
    "train_data.select(feature_columns).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f330ca1-83cf-461b-ac95-9da2b85fa7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "\n",
    "# Assemble all feature columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"Feat_vector1\")\n",
    "\n",
    "# Apply the assembler to your DataFrame to combine all features into a single vector\n",
    "final_data = assembler.transform(train_data)\n",
    "# Drop the specified columns\n",
    "final_data = final_data.drop(\"tokens\", \"NewFeatures\", \"features\")\n",
    "\n",
    "final_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7b034576-3443-4eb7-85f3-08a3eb95bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.drop(\"Feat_vector1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478cc42f-37da-4e1f-bd71-a9c81401c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8969b135-038e-4b1a-8068-8c5249b57dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features_vector1\", labelCol=\"stars\")\n",
    "\n",
    "# Train the decision tree model on the training data\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "feature_importances = dt_model.featureImportances.toArray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822d6b08-3a7d-4417-958a-454369336bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importances)), feature_importances)\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd1b53-62f0-4975-aae0-81959201f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of (feature_index, importance) tuples\n",
    "feature_importance_list = [(index, importance) for index, importance in enumerate(feature_importances)]\n",
    "\n",
    "# Sort the features by importance in descending order\n",
    "sorted_features = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted features\n",
    "print(\"Sorted Features by Importance:\")\n",
    "for feature in sorted_features:\n",
    "    print(f\"Feature Index: {feature[0]}, Importance: {feature[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6dea3eb5-b0ca-478a-86b7-de819ed0f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indices = [105, 405, 250, 204]\n",
    "selected_feature_names = [f'feature_{idx}' for idx in feature_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efc1478-c37b-4195-aa0c-eee073c1843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"stars\")\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on dev data\n",
    "dev_predictions = lr_model.transform(dev_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"stars\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = evaluator.evaluate(dev_predictions)\n",
    "print(\"Mean Squared Error on Dev Data for Linear Regression:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ca6d5-f071-46a7-aff4-1f0ed8031116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Train Random Forest model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"stars\")\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions on dev data\n",
    "dev_predictions = rf_model.transform(dev_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"stars\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = evaluator.evaluate(dev_predictions)\n",
    "print(\"Mean Squared Error on Dev Data for Random Forest:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7aef62-c3d1-4134-9d21-6fa16848494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "test_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Calculate Mean Squared Error on test data\n",
    "mse_test = evaluator.evaluate(test_predictions)\n",
    "print(\"Mean Squared Error on Test Data for Linear Regression:\", mse_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
